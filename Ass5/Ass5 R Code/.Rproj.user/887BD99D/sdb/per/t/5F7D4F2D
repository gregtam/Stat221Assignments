{
    "contents" : "dlogprior = function(eta,eta.hat,Sigma.hat)\n{\n  dmvnorm(eta, mean=eta.hat, sigma=Sigma.hat, log=TRUE)\n}\ndloglikelihood = function(Y,A,lambda,Sigma)\n{\n  dmvnorm(Y, mean=A %*% lambda, sigma= A %*% Sigma %*% t(A), log=TRUE)\n}\n\nftest = function(theta,a,b,c)\n{\n  #the function f which we try to set to 0\n  #takes theta as input, returns f(theta) vector\n  ans = c()\n  lambda = head(theta,length(theta)-1)\n  phi = tail(theta,1)\n  for(i in 1:length(theta))\n  {\n    if(i<=I)\n      ans[i] = c*phi*lambda[i]^c + (2-c)*lambda[i]^2 - 2*(1-c)*lambda[i]*b[i] - c*a[i]\n    if(i==I+1)\n      ans[i] = sum(lambda^(-c+1)*(lambda - b))\n  }\n  ans\n}\n\n\n\n\n#############\n#LOCAL MODEL#\n#############\n\nlocally_iid_EM = function(data, c, A, lambda.init = 1e6, phi.init=0.5)\n{\n  w=11\n  h=(w-1)/2\n  T=ncol(y)\n  #initialize the theta vector for simplicity, keep all lambdas the same. \n  #choice of these parameters are important. I initially had small lambdas, and\n  #the EM would not work.\n  lambda = rep(lambda.init,ncol(A))\n  phi = phi.init\n  theta = matrix(c(lambda,phi),ncol=1)\n  \n  f.array = matrix(,ncol=0,nrow=I+1)\n  #replicate for each t\n  for(t in 6:(T-5))\n  {\n    #previous theta vector (lambda, phi)\n    theta.old = theta[,ncol(theta)]\n    theta.new=c()\n    \n    #E-step\n    Sigma = diag(phi*lambda^c)\n    min.t = t-h\n    max.t = t+h\n    m = sapply(min.t:max.t, function(i) lambda + Sigma %*% t(A) %*% ginv(A %*% Sigma %*% t(A)) %*% (data[-nrow(data),i] - A %*% lambda))\n    R = Sigma - Sigma %*% t(A) %*% ginv(A %*% Sigma %*% t(A)) %*% A %*% Sigma\n    \n    #finds f, which is the derivative of Q(theta,theta^{(t)})\n    #Newton-Raphson is done on f to find the maximum of Q   \n    a=c()\n    b=c()\n    f=c()\n    for(i in 1:I)\n    {\n      a[i] = R[i,i] + mean(m[i,]^2)\n      b[i] = mean(m[i,])\n      f[i] = c*phi*lambda[i]^c + (2-c)*lambda[i]^2 - 2*(1-c)*lambda[i]*b[i] - c*a[i]\n    }\n    f[I+1] = sum(lambda^(-c+1)*(lambda - b))\n    \n    \n    #analytical solution: when c=2, it sets the lambda such that f()=0\n    lambda = (-b + sqrt(b^2 + 4*a*phi))/(2*phi)\n    #replace lambdas with analytical solutions\n    theta.new[1:I] = lambda\n    #check that f is close to 0\n    ftest(c(lambda,phi),a,b,c)\n    \n    #M-step\n    \n    #Create Fdot matrix (derivative)\n    Fdot = matrix(,nrow=I+1,ncol=I+1)\n    for(i in 1:I)\n      for(j in 1:I)\n        Fdot[i,j] = (i==j)*(phi*c^2*lambda[i]^(c-1) + 2*(2-c)*lambda[i] - 2*(1-c)*b[i])\n    for(j in 1:I)\n      Fdot[I+1,j] = (2-c)*lambda[j]^(1-c) - (1-c)*lambda[j]^(-c)*b[j]\n    for(i in 1:I)\n      Fdot[i,I+1] = c*lambda[i]^c\n    Fdot[I+1,I+1] = 0\n    \n    mult=1\n    tempInv = ginv(Fdot) %*% f\n    repeat\n    {\n      #Fractional Newton-Raphson: Shrinks amount it takes off to ensure phi remains greater than 0\n      phi = theta.old[I+1] - mult*tempInv[I+1,]\n      if(phi>0)\n        break\n      mult = mult/2\n    }\n    theta.new[I+1] = phi\n    ftest(c(lambda,phi),a,b,c)\n    \n    theta.old = theta.new\n    theta = cbind(theta,theta.new)\n    f.array = cbind(f.array,ftest(theta.new,a,b,c))\n  }\n  list(\"f\"=f.array,\"theta\"=theta,\"a\"=a,\"b\"=b)\n}\n\n\n################\n#SMOOTHED MODEL#\n################\n\nsmoothed_EM = function(data,c,A,V.init=5,eta.init=3)\n{\n  I=ncol(A)\n  J=nrow(A)+1\n  c=2\n  \n  #Step 1 (Set initial parameters)\n  h=5\n  w=2*h+1\n  t=h+1\n  eta = matrix(rep(eta.init,ncol(A)+1),nrow=ncol(A)+1,ncol=1)\n  V = diag(rep(V.init, ncol(A)+1))\n  f.array = matrix(,ncol=0,nrow=I+1)\n  \n  \n  #Step 2\n  for(t in 6:(T-5))\n  {\n    eta.hat.old = eta[,ncol(eta)]\n    eta.hat.new = c()\n    \n    #E-step\n    #set lambda and phi based on eta\n    lambda = exp(head(eta.hat.old,length(eta.hat.old)-1))\n    phi = exp(tail(eta.hat.old,1))\n    \n    Sigma.full = diag(c(phi*lambda^c, phi))\n    Sigma = diag(phi*lambda^c)\n    min.t = t-h\n    max.t = t+h\n    m = sapply(min.t:max.t, function(i) lambda + Sigma %*% t(A) %*% ginv(A %*% Sigma %*% t(A)) %*% (data[-nrow(data),i] - A %*% lambda))\n    R = Sigma - Sigma %*% t(A) %*% ginv(A %*% Sigma %*% t(A)) %*% A %*% Sigma\n    \n    Q.local = -w/2*(log(det(Sigma))) + tr(ginv(Sigma) %*% R) - 1/2*sum(sapply(1:11,function(i) t(m[,i]-lambda) %*% ginv(Sigma) %*% (m[,i]-lambda)))  \n    Q.prior = function(eta.val)\n    {\n      eta.hat.old\n      Sigma.hat = Sigma.full + V\n      dmvnorm(eta.val, mean=eta.hat.old, sigma=Sigma.hat, log=TRUE)\n    }\n    g = function(eta.val){Q.local + Q.prior(eta.val)}\n    \n    #Find mode of g\n    #Optimizing over all eta's at once doesn't work, so we do it termwise\n    g.marginal = function(eta.change, eta.val,index)\n    {\n      #eta.change: eta coordinate we change\n      #index: index for eta.change\n      #eta.val: vector of etas\n      eta.val[index]=eta.change\n      g(eta.val)\n    }\n    \n    eta.hat.new=eta.hat.old\n    eta.hat.new[1] = optim(1, g.marginal, eta.val=eta.hat.old, index=1, control=list(fnscale=-1),method=\"Brent\",lower=1.0001,upper=1e7)$par\n    for(reps in 1:20)\n    {\n      for(i in 1:17)\n      {\n        eta.hat.new[i] = optim(eta.hat.new[i], g.marginal, eta.val=eta.hat.new, index=i, control=list(fnscale=-1),method=\"Brent\",lower=1.0001,upper=1e7)$par\n      }\n    }\n    eta.hat.old = eta.hat.new\n    eta = cbind(eta,eta.hat.new)\n    f.array = cbind(f.array,exp(dlogprior(head(eta.hat.new,length(eta.hat.new)-1),head(eta.hat.old,length(eta.hat.old)-1),Sigma) + dloglikelihood(t(data[-nrow(data),t]),A,lambda,Sigma)))\n    print(paste(\"Step\",t))\n  }\n  list(\"f\"=f.array,\"eta\"=eta, \"theta\"=exp(eta))\n}\n\n\n\n\n\n",
    "created" : 1416531903434.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "586153559",
    "id" : "5F7D4F2D",
    "lastKnownWriteTime" : 1416537121,
    "path" : "~/Documents/School/Harvard/Third Semester/STAT 221/Assignments/Ass5/Ass5 R Code/tam_gregory_functions.R",
    "project_path" : "tam_gregory_functions.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}